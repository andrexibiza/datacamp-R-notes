---
title: "Intermediate Regression in R"
output: html_notebook
---

# DataCamp: Intermediate Regression in R

Linear regression and logistic regression are the two most widely used statistical models and act like master keys, unlocking the secrets hidden in datasets. This course builds on the skills you gained in "Introduction to Regression in R", covering linear and logistic regression with multiple explanatory variables. Through hands-on exercises, you’ll explore the relationships between variables in real-world datasets, Taiwan house prices and customer churn modeling, and more. By the end of this course, you’ll know how to include multiple explanatory variables in a model, understand how interactions between variables affect predictions, and understand how linear and logistic regression work.

## Challenging Data Setup

The [course page](https://app.datacamp.com/learn/courses/intermediate-regression-in-r) contains three files:

1)  Taiwan real estate (`taiwan_real_estate2.fst`)

2)  eBay Palm Pilot auctions (`create-ebay-palm-pilot-auctions-dataset.R`)

3)  Bank churn (`churn.fst`)

Download the files into your `data` folder for your project.Import `.fst` files

`.fst` files are a fast and efficient file format for storing large data frames in R. They offer high compression and rapid read/write speeds - `.fst` files provide faster read/write operations and better compression compared to traditional formats like `.csv`, making them suitable for large datasets. To work with `.fst` files in R, you can use the `fst` package. Here's a brief guide:

-   Installation: Install the `fst` package if you haven't already.

```         
install.packages("fst")
```

-   Reading `.fst` Files: Use the `read_fst` function to load data from an `.fst` file.

```         
library(fst)
data <- read_fst("path/to/your/file.fst")
```

-   Writing `.fst` Files: Use the `write_fst` function to save a data frame to an `.fst` file.

```         
write_fst(data, "path/to/your/file.fst")
```

### eBay Palm Pilot auctions

The file on the course page includes the following script.

```         
# Downloaded from https://www.kaggle.com/onlineauctions/online-auctions-dataset
library(readr)
library(fst)

auctions_raw <- read_csv("~/Downloads/361_760_bundle_archive/auction.csv")

# Palm pilot has most data, no outliers
auctions <- auctions_raw %>%
  filter(item == "Palm Pilot M515 PDA") %>%
  select(price, openbid, auction_type)

write_fst(auctions, "auctions.fst")
```

The problem is that the file path `"~/Downloads/361_760_bundle_archive/auction.csv"` takes us nowhere.

```         
Error: '~/Downloads/361_760_bundle_archive/auction.csv' does not exist.
```

You first have to download the `.csv` file from the [Kaggle Online Auctions Dataset](https://www.kaggle.com/onlineauctions/online-auctions-dataset) and adjust the path. For all of my projects, I prefer to keep my source data files in a `data` folder within the project directory. Lastly, run the script to create the `.fst` file. The `read_fst` function will then load the data into R.

```{r}
# Load packages
library(fst)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)

# Load course data
churn <- read.fst("data/churn.fst")
taiwan_real_estate <- read.fst("data/taiwan_real_estate2.fst")

#---Online Auctions dataset R script from course page----
# Dataset downloaded from https://www.kaggle.com/onlineauctions/online-auctions-dataset in .csv format
auctions_raw <- read_csv("data/auction.csv")

# Palm pilot has most data, no outliers
auctions <- auctions_raw %>%
  filter(item == "Palm Pilot M515 PDA") %>%
  select(price, openbid, auction_type)

write_fst(auctions, "data/auctions.fst") # for consistency
#----
```

## Additional datasets

### `fish`

The first video in the course introduces a `fish` dataset not provided in the course materials. Using a Google search, I found a match for the same dataset on [Kaggle](https://www.kaggle.com/datasets/vipullrathod/fish-market). The following script transforms the `Fish.csv` file from Kaggle into a data frame to match the course materials.

```{r}
# `fish` data from https://www.kaggle.com/datasets/vipullrathod/fish-market
# Filtering Kaggle data for species of interest to match course materials
fish <- read_csv("data/fish.csv") %>%
  select(Species, Weight, Length3) %>%
  rename(species = Species,
         mass_g = Weight,
         length_cm = Length3) %>%
  filter(species %in% c("Bream", "Roach", "Perch", "Pike"))
fish  

```

### `simpsons_paradox`

For video 2.4, we need a dataset called `simpsons_paradox`, which is no longer directly available through CRAN for R version 4.4.2. We can simulate this data using the `simulate_simpson` function from the `bayestestR` package. To Install the package, run `install.packages("bayestestR")`.

```{r}
library(bayestestR)
simpsons_paradox <- simulate_simpson(
  n = 100,
  r = 0.5,
  groups = 5,
  difference = 1,
  group_prefix = ""
) %>% 
  rename(
    x = V1,
    y = V2,
    group = Group
  ) %>% 
  mutate(group = recode(group, 
                        `1` = "A", 
                        `2` = "B", 
                        `3` = "C", 
                        `4` = "D", 
                        `5` = "E")
         )
```

```{r}
head(simpsons_paradox)
```

Now we are ready to begin Chapter 1!

# Chapter 1: Parallel Slopes

Extend your linear regression skills to "parallel slopes" regression, with one numeric and one categorical explanatory variable. This is the first step towards conquering multiple linear regression.

## Video 1.1: Parallel slopes linear regression

### From simple regression to multiple regression

-   *Multiple regression* is a regression model with more than one explanatory variable.
-   More explanatory variables can give **more insight** and **better predictions**.

### Course contents

**Chapter 1** - Parallel slopes regression

**Chapter 2** - Interactions - Simpson's Paradox

**Chapter 3** - More explanatory variables - How linear regression works

**Chapter 4** - Multiple logistic regression - The logistic distribution - How logistic regression works

### The `fish` dataset

-   Each row represents a fish
-   `mass_g` is the response variable
-   1 numeric, 1 categorical explanatory variable

### One explanatory variable at a time

```{r}
mdl_mass_vs_length <- lm(mass_g ~ length_cm, data = fish)
mdl_mass_vs_length
```

-   1 intercept coefficient
-   1 slope coefficient

```{r}
mdl_mass_vs_species <- lm(mass_g ~ species + 0, data = fish)
mdl_mass_vs_species
```

-   1 intercept coefficient for each category

### Both variables at the same time

```{r}
mdl_mass_vs_both <- lm(mass_g ~ length_cm + species + 0, data = fish)
mdl_mass_vs_both
```

-   1 slope coefficient
-   1 intercept coefficient for each category

### Comparing coefficients

```{r}
coefficients(mdl_mass_vs_length)
```

```{r}
coefficients(mdl_mass_vs_species)
```

```{r}
coefficients(mdl_mass_vs_both)
```

### Visualization: 1 numeric explanatory var

```{r}
ggplot(fish, aes(length_cm, mass_g)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

### Visualization: 1 categorical explanatory var

```{r}
ggplot(fish, aes(species, mass_g)) +
  geom_boxplot() +
  stat_summary(fun.y = mean, shape = 15) # shape makes mean points square
```

### Visualization: Both explanatory vars

```{r}
library(moderndive)
ggplot(fish, aes(length_cm, mass_g, color = species)) +
  geom_point() +
  geom_parallel_slopes(se = FALSE)
```

## **Fitting a parallel slopes linear regression**

In [**Introduction to Regression in R**](https://learn.datacamp.com/courses/introduction-to-regression-in-r), you learned to fit linear regression models with a single explanatory variable. In many cases, using only one explanatory variable limits the accuracy of predictions. That means that to truly master linear regression, you need to be able to include multiple explanatory variables.

The case when there is one numeric explanatory variable and one categorical explanatory variable is sometimes called a "parallel slopes" linear regression due to the shape of the predictions—more on that in the next exercise.

Here, you'll revisit the Taiwan real estate dataset. Recall the meaning of each variable.

| Variable | Meaning |
|:-----------------------------------|:-----------------------------------|
| `dist_to_mrt_station_m` | Distance to nearest MRT metro station, in meters. |
| `n_convenience` | No. of convenience stores in walking distance. |
| `house_age_years` | The age of the house, in years, in 3 groups. |
| `price_twd_msq` | House price per unit area, in New Taiwan dollars per meter squared. |

`taiwan_real_estate` is available.

**Instructions 1/3**

-   Using the `taiwan_real_estate` dataset, model the house price (in TWD per square meter) versus the number of nearby convenience stores.

```{r}
# Fit a linear regr'n of price_twd_msq vs. n_convenience
mdl_price_vs_conv <- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)

# See the result
mdl_price_vs_conv
```

**2/2:** Model the house price (in TWD per square meter) versus the house age (in years). Don't include an intercept term.

```{r}
# Fit a linear regr'n of price_twd_msq vs. house_age_years, no intercept
mdl_price_vs_age <- lm(price_twd_msq ~ house_age_years - 1, data = taiwan_real_estate)

# See the result
mdl_price_vs_age
```

**3/3**: Model the house price (in TWD per square meter) versus the number of nearby convenience stores plus the house age (in years). Don't include an intercept term.

```{r}
# Fit a linear regr'n of price_twd_msq vs. n_convenience plus house_age_years, no intercept
mdl_price_vs_both <- lm(price_twd_msq ~ n_convenience + house_age_years - 1, data = taiwan_real_estate)

# See the result
mdl_price_vs_both
```

## **Interpreting parallel slopes coefficients**

For linear regression with a single numeric explanatory variable, there is an intercept coefficient and a slope coefficient. For linear regression with a single categorical explanatory variable, there is an intercept coefficient for each category.

In the "parallel slopes" case, where you have a numeric and a categorical explanatory variable, what do the coefficients mean?

## **Visualizing each explanatory variable**

Being able to see the predictions made by a model makes it easier to understand. In the case where there is only one explanatory variable, ggplot lets you do this without any manual calculation or messing about.

To visualize the relationship between a numeric explanatory variable and the numeric response, you can draw a scatter plot with a linear trend line.

To visualize the relationship between a categorical explanatory variable and the numeric response, you can draw a box plot.

`taiwan_real_estate` is available and `ggplot2` is loaded.

**Instructions 1/2**

-   Using the `taiwan_real_estate` dataset, plot the house price versus the number of nearby convenience stores.

-   Make it a scatter plot.

-   Add a smooth linear regression trend line without a standard error ribbon.

```{r}
# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience
ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) +
  # Add a point layer
  geom_point() +
  # Add a smooth trend line using linear regr'n, no ribbon
  geom_smooth(method = "lm", se = FALSE)
```

**2/2**

-   Using the `taiwan_real_estate` dataset, plot the house price versus the house age.

-   Make it a box plot.

```{r}
# Using taiwan_real_estate, plot price_twd_msq vs. house_age_years
ggplot(taiwan_real_estate, aes(house_age_years, price_twd_msq)) +
  # Add a box plot layer
  geom_boxplot()
```

## **Visualizing parallel slopes**

The two plots in the previous exercise gave very different predictions: one gave a predicted response that increased linearly with a numeric variable; the other gave a fixed response for each category. The only sensible way to reconcile these two conflicting predictions is to incorporate both explanatory variables in the model at once.

When it comes to a linear regression model with a numeric and a categorical explanatory variable, `ggplot2` doesn't have an easy, "out of the box" way to show the predictions. Fortunately, the `moderndive` package includes an extra geom, `geom_parallel_slopes()` to make it simple.

`taiwan_real_estate` is available; `ggplot2` and `moderndive` are loaded.

**Instructions**

-   Using the `taiwan_real_estate` dataset, plot house prices versus the number of nearby convenience stores, colored by house age.

-   Make it a scatter plot.

-   Add parallel slopes, without a standard error ribbon.

```{r}
library(moderndive)

# Using taiwan_real_estate, plot price_twd_msq vs. n_convenience colored by house_age_years
ggplot(taiwan_real_estate, aes(x = n_convenience, y = price_twd_msq, color = house_age_years)) +
  # Add a point layer
  geom_point() +
  # Add parallel slopes, no ribbon
  geom_parallel_slopes(se = FALSE)
```

## Video 1.2: Predicting Parallel Slopes

Predicting responses is perhaps the most useful feature of regression models. With two explanatory variables, the code for prediction has one subtle difference from the case with a single explanatory variable.

### The prediction workflow

The prediction workflow starts with choosing values for explanatory variables. You pick any values you want, and store them in a data frame or tibble. For a single explanatory variable, the data frame has one column. Here, it's a sequence of lengths from 5cm to 60cm, in steps of 5cm. For multiple explanatory variables, it's the same process, but there's a useful trick. `expand_grid` from the `tidyr` package returns a data frame of all combinations of its inputs. Here, you have 5cm and each fish species, 10cm and each fish species, through to 60cm and each fish species.

```{r}
explanatory_data <- tibble(
  length_cm = seq(5, 60, 5)
)
glimpse(explanatory_data)
```

```{r}
explanatory_data <- expand_grid(
  length_cm = seq(5, 60, 5),
  species = unique(fish$species)
)
glimpse(explanatory_data)
```

Next you add a column of predictions to the data frame. To calculate the predictions, call `predict`, passing the model and the explanatory data. Here's the code for one explanatory variable. With two or more explanatory variables, other than the model variable name, the code is exactly the same!

```{r}
prediction_data <- explanatory_data %>%
  mutate(
    mass_g = predict(mdl_mass_vs_length, explanatory_data
                     )
  )
```

```{r}
prediction_data <- explanatory_data %>%
  mutate(
    mass_g = predict(mdl_mass_vs_both, explanatory_data
                     )
  )
prediction_data
```

### Visualizing the predictions

Just as in the single explanatory variable case, we can visualize the predictions from the model by adding another `geom_point` layer and setting the data argument to prediction_data. I set the size and shape arguments to make the predictions big square points. A good sign that this worked is that the prediction points lie along the lines calculated by ggplot.

```{r}
library(ggplot2)
library(moderndive)
ggplot(fish, aes(length_cm, mass_g, color = species)) +
  geom_point() +
  geom_parallel_slopes(se = FALSE) +
  geom_point(
    data = prediction_data,
    size = 3, shape = 15
  )
```

### Manually calculating predictions

In the previous course, you saw how to manually calculate the predictions. The `coefficients` function extracts the coefficients from the model. The intercept is the first coefficient, and the slope is the second coefficient. Then the response value is the intercept plus the slope times the explanatory variable.

```{r}
coeffs <- coefficients(mdl_mass_vs_length)
coeffs
```

Then the response value is the intercept plus the slope times the explanatory variable.

```{r}
intercept <- coeffs[1]
slope <- coeffs[2]

explanatory_data %>% 
  mutate(
    mass_g = intercept + slope * length_cm
  )
explanatory_data
```

### Coefficients for parallel slopes

For the parallel slopes model, there is an added complication. Each category of the categorical variable has a different intercept. Due to the way we specified the model, the slope coefficient is the first one.

```{r}
coefficients(mdl_mass_vs_both)
```

```{r}
slope <- coeffs[1]
intercept_bream <- coeffs[2]
intercept_perch <- coeffs[3]
intercept_pike <- coeffs[4]
intercept_roach <- coeffs[5]
```

### Choosing an intercept with `ifelse()`

You can choose this intercept using `if-else` statements, but this becomes clunky when you have lots of categories. With just four categories, these nested calls to `ifelse` are hard to write and hard to read. It's a recipe for buggy code.

```         
explanatory_data %>%  
  mutate(    
    intercept = ifelse(      
      species =="Bream",      
      intercept_bream,      
      ifelse(        
        species =="Perch",        
        intercept_perch,        
        ifelse(          
          species =="Pike",          
          intercept_pike,          
          intercept_roach
          )
        )
      )
    )
```

### `case_when()`

`dplyr` has a function called `case_when` that simplifies the code. Each argument to `case_when` is a formula, just like you use when specifying a model. On the left-hand side, you have a logical condition. On the right-hand side, you have the value to give to those rows where the condition is met. This is very abstract, so let's look at how we use it for predictions.

```         
dataframe %>% 
    mutate(
      case_when(
        condition_1 ~ value_1,
        condition_2 ~ value_2,
        # ...
        condition_n ~ value_n
      )
    )
```

### Choosing an intercept with `case_when()`

The first argument to `case_when` has a logical condition to check for rows where the species is Bream. On the right-hand side of the formula, we give those rows the value of the bream intercept. Then we repeat this for the other species. This code does the same thing as the `ifelse` code, but I find it easier to write and to read.

```         
explanatory_data %>%  
  mutate(    
    intercept = case_when(      
      species == "Bream" ~ intercept_bream,      
      species == "Perch" ~ intercept_perch,      
      species == "Pike" ~ intercept_pike,      
      species == "Roach" ~ intercept_roach
      )
    )
```

### The final prediction step

The final step is to calculate the response. As before, the response is the intercept plus the slope times the numeric explanatory variable. This time, the intercept is different for different rows.

```{r}
explanatory_data %>% 
  mutate(
    intercept = case_when(
      species == "Bream" ~ intercept_bream,
      species == "Perch" ~ intercept_perch,
      species == "Pike" ~ intercept_pike,
      species == "Roach" ~ intercept_roach
    ),
    mass_g = intercept + slope * length_cm
  )
```

### Compare to `predict()`

The model predicts some negative masses, which isn't a good sign. Let's check that we got the right answer by calling predict. You can see that the predictions are the same numbers as the mass column that we calculated, so our calculations are correct. It's just that this model performs poorly for small fish lengths.

```{r}
predict(mdl_mass_vs_both, explanatory_data)
```

## **Predicting with a parallel slopes model**

While ggplot can automatically show you model predictions, in order to get those values to program with, you'll need to do the calculations yourself.

Just as with the case of a single explanatory variable, the workflow has two steps: create a data frame of explanatory variables, then add a column of predictions. To make sure you've got the right answer, you can add your predictions to the ggplot with the `geom_parallel_slopes()` lines.

`taiwan_real_estate` and `mdl_price_vs_both` are available; `dplyr`, `tidyr`, and `ggplot2` are loaded.

**Instructions 1/3**

Make a grid of explanatory data, formed from combinations of the following variables.

-   `n_convenience` should take the numbers zero to ten.

-   `house_age_years` should take the unique values of the `house_age_years` column of `taiwan_real_estate`.

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

# Make a grid of explanatory data
explanatory_data <- expand_grid(
  # Set n_convenience to zero to ten
  n_convenience = 0:10,
  # Set house_age_years to the unique values of that variable
  house_age_years = unique(taiwan_real_estate$house_age_years)
)

# See the result
explanatory_data
```

-   **2/3:** Add a column to the `explanatory_data` named for the response variable, assigning to `prediction_data`.

-   The response column contain predictions made using `mdl_price_vs_both` and `explanatory_data`.

```{r}
# From previous step
explanatory_data <- expand_grid(
  n_convenience = 0:10,
  house_age_years = unique(taiwan_real_estate$house_age_years)
)

# Add predictions to the data frame
prediction_data <- explanatory_data %>% mutate(
  proce_twd_msq = predict(mdl_price_vs_both, newdata = explanatory_data)
)
# See the result
prediction_data
```

**3/3**

-   Update the plot to add a point layer of predictions. Use the `prediction_data`, set the point size to 5, and the point shape to 15.

```{r}
# From previous steps
explanatory_data <- expand_grid(
  n_convenience = 0:10,
  house_age_years = unique(taiwan_real_estate$house_age_years)
)
prediction_data <- explanatory_data %>% 
  mutate(
    price_twd_msq = predict(mdl_price_vs_both, explanatory_data)
  )

taiwan_real_estate %>% 
  ggplot(aes(n_convenience, price_twd_msq, color = house_age_years)) +
  geom_point() +
  geom_parallel_slopes(se = FALSE) +
  # Add points using prediction_data, with size 5 and shape 15
  geom_point(
    data = prediction_data,
    size = 5,
    shape = 15
  )
```

## **Manually calculating predictions**

As with simple linear regression, you can manually calculate the predictions from the model coefficients. The only change for the parallel slopes case is that the intercept is different for each category of the categorical explanatory variable. That means you need to consider the case when each each category occurs separately.

`taiwan_real_estate`, `mdl_price_vs_both`, and `explanatory_data` are available; `dplyr` is loaded.

**Instructions 1/2**

-   Get the coefficients from `mdl_price_vs_both`, assigning to `coeffs`.

-   Assign each of the elements of `coeffs` to the appropriate variable.

```{r}
# Get the coefficients from mdl_price_vs_both
coeffs <- coefficients(mdl_price_vs_both)

# Extract the slope coefficient
slope <- coeffs[1]

# Extract the intercept coefficient for 0 to 15
intercept_0_15 <- coeffs[2]

# Extract the intercept coefficient for 15 to 30
intercept_15_30 <- coeffs[3]

# Extract the intercept coefficient for 30 to 45
intercept_30_45 <- coeffs[4]
```

**2/2**

Add columns to `explanatory_data`.

-   To choose the `intercept`, in the case when `house_age_years` is `"0 to 15"`, choose `intercept_0_15`. In the case when `house_age_years` is `"15 to 30"`, choose `intercept_15_30`. Do likewise for `"30 to 45"`.

-   Manually calculate the predictions as the intercept plus the slope times `n_convenience`.

```{r}
# From previous step
coeffs <- coefficients(mdl_price_vs_both)
slope <- coeffs[1]
intercept_0_15 <- coeffs[2]
intercept_15_30 <- coeffs[3]
intercept_30_45 <- coeffs[4]

prediction_data <- explanatory_data %>% 
  mutate(
    # Consider the 3 cases to choose the intercept
    intercept = case_when(
      house_age_years == "0 to 15" ~ intercept_0_15,
      house_age_years == "15 to 30" ~ intercept_15_30,
      house_age_years == "30 to 45" ~ intercept_30_45
    ),    
    
    # Manually calculate the predictions
    price_twd_msq = intercept + slope * n_convenience
  )

# See the results
prediction_data
```

## Video 1.3: Assessing Model Performance

The big benefit of using more than one explanatory variable in a model is that you can sometimes get a better fit than when you use a single explanatory variable.

### Model performance metrics

-   *Coefficient of determination (R-squared)*: how well the linear regression fits the observed values.

    -   Larger is better.

-   *Residual Standard Error (RSE)*: the typical size of the residuals.

    -   Smaller is better.

In the last course, you saw two metrics for measuring model performance: the coefficient of determination, and the residual standard error. The coefficient of determination, sometimes known as the R-squared value, measures how good the regression's prediction line fits the observed values, and a larger number is better. The residual standard error, sometimes abbreviated to RSE, is - loosely speaking - the typical size of the residuals. Let's see if these metrics improve when both explanatory variables are included in the fish model.

### Getting the coefficient of determination

To easily get the coefficient of determination, load the `dplyr` and `broom` packages. Recall that `broom`'s `glance` function retrieves model-level metrics as a data frame. Then `dplyr`'s `pull` function can be used to extract the metric we want. The coefficient of determination is called `r.squared`. For the mass versus species model, the coefficient of determination is `0.72`, where `0` is the worst possible fit and `1` is a perfect fit. For the mass versus length model, the coefficient of determination is better, at `0.82`. For the mass versus both model, the coefficient of determination is even higher, at `0.97`. Using this metric, the model with both explanatory variables is the best one, since it has the highest coefficient of determination.

```{r}
library(dplyr)
library(broom)
```

```{r}
mdl_mass_vs_length %>% 
  glance() %>% 
  pull(r.squared)
```

```{r}
mdl_mass_vs_species %>% 
  glance() %>% 
  pull(r.squared)
```

```{r}
mdl_mass_vs_both %>% 
  glance() %>% 
  pull(r.squared)
```

### Adjusted coefficient of determination

Adding more explanatory variables often increases the coefficient of determination for a model, but there is a problem. Including too many explanatory variables in your model can lead to a phenomenon called overfitting. That's when your model is optimized to provide the best fit for that particular dataset, but no longer reflects the general population. In this case, the model would be overfit if it performed well on this fish dataset, but badly on a different fish dataset. A variant metric called adjusted coefficient of determination includes a small penalty term for each additional explanatory variable to compensate for this effect. Its a better metric than the plain coefficient of determination. Its equation is based on the plain coefficient of determination, the number of observations, and the number of explanatory variables, including interactions. The penalty is big enough to worry about if the plain coefficient of determination is small, or if the number of explanatory variables is a sizable fraction of the number of observations. To get this metric, we retrieve the `adj.r.squared` element from the glanced model.

-   More explanatory variables increases $R^2$.

-   Too many explanatory variables causes **overfitting**.

-   *Adjusted coefficient of determination* penalizes more explanatory variables.

$$
\bar{R}^2 = 1 - (1 - R^2)\frac{n_{obs} -1}{n_{obs}-n_{var}-1}
$$

-   Penalty is noticeable when $R^2$ is small or $n_{var}$ is a large fraction of $n_{obs}$.

-   In `glance()`, it's the `adj.r.squared` element.

### Getting the adjusted coefficient of determination

To see the effect of penalization, let's look at the unadjusted and adjusted coefficients side-by-side. Since each model only contains one or two explanatory variables, the effect is tiny.

```{r}
library(dplyr)
library(broom)
```

```{r}
mdl_mass_vs_length %>% 
  glance() %>% 
  select(r.squared, adj.r.squared)
```

```{r}
mdl_mass_vs_species %>% 
  glance() %>% 
  select(r.squared, adj.r.squared)
```

```{r}
mdl_mass_vs_both %>% 
  glance() %>% 
  select(r.squared, adj.r.squared)
```

### Getting the Residual Standard Error

The code to get the residual standard error is the same as before, but this time we pull out sigma. The mass versus species model has an RSE of just over three hundred. The mass versus length model has an RSE of about one hundred and fifty. Finally, the mass versus both model has an RSE of just over one hundred, meaning that it typically gets the mass wrong by about one hundred grams. Since that number is the lowest of the three, by this metric, the mass versus both model is best. That means that all metrics indicate that the model with two explanatory variables is better than the models with just one explanatory variable.

```{r}
library(dplyr)
library(broom)
```

```{r}
mdl_mass_vs_length %>% 
  glance %>% 
  pull(sigma)
```

```{r}
mdl_mass_vs_species %>% 
  glance %>% 
  pull(sigma)
```

```{r}
mdl_mass_vs_both %>% 
  glance %>% 
  pull(sigma)
```

## **Comparing coefficients of determination**

Recall that the coefficient of determination is a measure of how well the linear regression line fits the observed values. An important motivation for including several explanatory variables in a linear regression is that you can improve the fit compared to considering only a single explanatory variable.

Here you'll compare the coefficient of determination for the three Taiwan house price models, to see which gives the best result.

`mdl_price_vs_conv`, `mdl_price_vs_age`, and `mdl_price_vs_both` are available; `dplyr` and `broom` are loaded.

**Instructions 1/2**

-   Get the unadjusted and adjusted coefficients of determination for `mdl_price_vs_conv` by glancing at the model, then selecting the `r.squared` and `adj.r.squared` values.

-   Do the same for `mdl_price_vs_age` and `mdl_price_vs_both`.

```{r}
mdl_price_vs_conv %>% 
  # Get the model-level coefficients
  glance() %>% 
  # Select the coeffs of determination
  select(r.squared, adj.r.squared)

# Get the coeffs of determination for mdl_price_vs_age
mdl_price_vs_age %>% 
  glance() %>% 
  select(r.squared, adj.r.squared)

# Get the coeffs of determination for mdl_price_vs_both
mdl_price_vs_both %>% 
  glance() %>% 
  select(r.squared, adj.r.squared)
```

## **Comparing residual standard error**

The other common metric for assessing model fit is the residual standard error (RSE), which measures the typical size of the residuals.

In the last exercise you saw how including both explanatory variables into the model increased the coefficient of determination. How do you think using both explanatory variables will change the RSE?

`mdl_price_vs_conv`, `mdl_price_vs_age`, and `mdl_price_vs_both` are available; `dplyr` and `broom` are loaded.

**Instructions 1/2**

-   Get the residual standard error for `mdl_price_vs_conv` by glancing at the model, then pulling the `sigma` value.

-   Do the same for `mdl_price_vs_age`.

-   Do the same for `mdl_price_vs_both`.

```{r}
mdl_price_vs_conv %>% 
  # Get the model-level coefficients
  glance() %>% 
  # Pull out the RSE
  pull(sigma)

# Get the RSE for mdl_price_vs_age
mdl_price_vs_age %>% 
  glance() %>%
  pull(sigma)


# Get the RSE for mdl_price_vs_both
mdl_price_vs_both %>% 
  glance() %>%
  pull(sigma)
```

`mdl_price_vs_both` gives the most accurate predictions.

# Chapter 2: Interactions

Explore the effect of interactions between explanatory variables. Considering interactions allows for more realistic models that can have better predictive power. You'll also deal with Simpson's Paradox: a non-intuitive result that arises when you have multiple explanatory variables.

## Video 2.1: Models for each category

The parallel slopes model enforced a common slope for each category. That's not always the best option.

### 4 categories

Recall that the fish dataset had four different species of fish. One way to give each species a different slope is to run a separate model for each of these.

```{r}
unique(fish$species)
```

### Splitting the dataset

There are many smart ways of splitting a dataset into parts and computing on each part. In base-R, you can use `split` and `lapply`. In `dplyr`, you can use `nest_by` and `mutate`. We aren't going to do that. Instead, let's `filter` for each species one at a time and assign the result to individual variables. I'm choosing this approach partly because I don't want fancy code to get in the way of reasoning about models, and partly because **running regression models is such a fundamental task that you need to be able to write the code without thinking, and that takes practice.** With this approach, you'll be writing the modeling code for every category in the dataset.

**The smart way**

-   base-R: `split()` + `lapply()`

-   `dplyr`: `nest_by()` + `mutate()`

**The simple way**

```{r}
bream <- fish %>%  
    filter(species =="Bream")
perch <- fish %>%  
    filter(species =="Perch")
pike <- fish %>%  
    filter(species =="Pike")
roach <- fish %>%  
    filter(species =="Roach")
```

### 4 models

Now we have four datasets, we can run four models. Again, there's no fancy looping, just the same model four times. Observe that each model gives a different intercept and a different slope.

```{r}
mdl_bream <- lm(mass_g ~ length_cm, data = bream)
mdl_bream
```

```{r}
mdl_pike <- lm(mass_g ~ length_cm, data = pike)
mdl_pike
```

```{r}
mdl_perch <- lm(mass_g ~ length_cm, data = perch)
mdl_perch
```

```{r}
mdl_roach <- lm(mass_g ~ length_cm, data = roach)
mdl_roach
```

### Explanatory data

To make predictions with these models, we first have to create a data frame of explanatory variables. The good news is that since each model has the same explanatory variable, you only have to write this code once. Give your copy and paste fingers a rest.

```{r}
explanatory_data <- tibble(
  length_cm = seq(5, 60, 5)
)
```

### Making predictions

Predicting follows the now familiar flow. Add a column with `mutate`, name it after the response variable, call `predict` with the model and the explanatory data. The only difference in each case is the `model` variable. It isn't necessary for calculating the predictions, but to make the plotting code you are about to see easier, I've also included the species in each prediction dataset.

```{r}
prediction_data_bream <- explanatory_data %>%  
  mutate(    
    mass_g = predict(mdl_bream, explanatory_data),    
    species ="Bream"
    )
```

```{r}
prediction_data_pike <- explanatory_data %>% 
  mutate(
    mass_g = predict(mdl_pike, explanatory_data),
    species = "Pike"
  )
```

```{r}
prediction_data_perch <- explanatory_data %>% 
  mutate(
    mass_g = predict(mdl_perch, explanatory_data),
    species = "Perch"
  )
```

```{r}
prediction_data_roach <- explanatory_data %>% 
  mutate(
    mass_g = predict(mdl_roach, explanatory_data),
    species = "Roach"
  )
```

### Visualizing predictions

Here's the standard `ggplot` for showing linear regression predictions. `geom_point` makes it a scatter plot, and `geom_smooth` with `method = "lm"` provides prediction lines. Unlike the parallel slopes case, each line has its own slope. This is achieved by setting the color aesthetic.

```{r}
ggplot(fish, aes(length_cm, mass_g, color = species)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

### Adding in your predictions

To sanity check our predictions, we add them to the plot to see if they align with what `ggplot2` calculated. The size and shape are changed to help them stand out. Thankfully, each line of squares follows `ggplot`'s trend lines.

```{r}
ggplot(fish, aes(length_cm, mass_g, color = species)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = prediction_data_bream, size = 3, shape = 15) +
  geom_point(data = prediction_data_perch, size = 3, shape = 15) +
  geom_point(data = prediction_data_pike, size = 3, shape = 15) +
  geom_point(data = prediction_data_roach, size = 3, shape = 15)
```

### Coefficient of determination

An important question here is **are these models better**? The coefficient of determination for a model on the whole `fish` dataset is `0.92`. Now here's the coefficient of determination for each of the individual models. The pike number is higher, indicating a better fit, though the numbers for the other models are lower.

```{r}
mdl_fish <- lm(mass_g ~ length_cm + species, data = fish)

mdl_fish %>% 
  glance() %>% 
  pull(adj.r.squared)
```

```{r}
mdl_bream %>% 
  glance() %>% 
  pull(adj.r.squared)
```

```{r}
mdl_pike %>% 
  glance() %>% 
  pull(adj.r.squared)
```

```{r}
mdl_roach %>% 
  glance() %>% 
  pull(adj.r.squared)
```

### Residual standard error

Here's the residual standard error for the whole dataset model, one hundred and three. For the individual models, this time the pike residual standard error is higher, indicating larger differences between actual and predicted values, but the other models show an improvement over the whole dataset model. This mixed performance result is quite common: the whole dataset model benefits from the increased power of more rows of data, whereas individual models benefit from not having to satisfy differing components of data.

```{r}
mdl_fish %>% 
  glance() %>% 
  pull(sigma)
```

```{r}
mdl_bream %>% 
  glance() %>% 
  pull(sigma)
```

```{r}
mdl_pike %>% 
  glance() %>% 
  pull(sigma)
```

```{r}
mdl_roach %>% 
  glance() %>% 
  pull(sigma)
```

## **One model per category**

The model you ran on the whole dataset fits some parts of the data better than others. It's worth taking a look at what happens when you run a linear model on different parts of the dataset separately, to see if each model agrees or disagrees with the others.

`taiwan_real_estate` is available; `dplyr` is loaded.

**Instructions 1/2**

-   Filter `taiwan_real_estate` for rows where `house_age_years` is `"0 to 15"`, assigning to `taiwan_0_to_15`.

-   Repeat this for the `"15 to 30"` and `"30 to 45"` house age categories.

```{r}
# Filter for rows where house age is 0 to 15 years
taiwan_0_to_15 <- taiwan_real_estate %>% 
    filter(house_age_years == "0 to 15")

# Filter for rows where house age is 15 to 30 years
taiwan_15_to_30 <- taiwan_real_estate %>% 
    filter(house_age_years == "15 to 30")

# Filter for rows where house age is 30 to 45 years
taiwan_30_to_45 <- taiwan_real_estate %>% 
    filter(house_age_years == "30 to 45")
```

**2/2**

-   Run a linear regression of `price_twd_msq` versus `n_convenience` using the `taiwan_0_to_15` dataset.

-   Repeat this for `taiwan_15_to_30` and `taiwan_30_to_45`.

```{r}
# Model price vs. no. convenience stores using 0 to 15 data
mdl_0_to_15 <- lm(price_twd_msq ~ n_convenience, data = taiwan_0_to_15)

# Model price vs. no. convenience stores using 15 to 30 data
mdl_15_to_30 <- lm(price_twd_msq ~ n_convenience, data = taiwan_15_to_30)

# Model price vs. no. convenience stores using 30 to 45 data
mdl_30_to_45 <- lm(price_twd_msq ~ n_convenience, data = taiwan_30_to_45)

# See the results
mdl_0_to_15
mdl_15_to_30
mdl_30_to_45
```

## **Predicting multiple models**

In order to see what each of the models for individual categories are doing, it's helpful to make predictions from them. The flow is exactly the same as the flow for making predictions on the whole model, though remember that you only have a single explanatory variable in these models (so `expand_grid()` isn't needed.)

The models `mdl_0_to_15`, `mdl_15_to_30` and `mdl_30_to_45` are available; `dplyr` is loaded.

**Instructions 1/2**

-   Create a tibble of explanatory data, setting `n_convenience` to a vector from zero to ten, assigning to `explanatory_data_0_to_15`.

```{r}
explanatory_data <- tibble(
  n_convenience = 0:10
)
```

**2/2**

-   Add a column of predictions named `price_twd_msq` to `explanatory_data`, using `mdl_0_to_15` and `explanatory_data`. Assign to `prediction_data_0_to_15`.

-   Repeat this for the 15 to 30 year and 30 to 45 year house age categories.

```{r}
# Add column of predictions using "0 to 15" model and explanatory data 
prediction_data_0_to_15 <- explanatory_data %>% 
  mutate(price_twd_msq = predict(mdl_0_to_15, newdata = explanatory_data))

# Same again, with "15 to 30"
prediction_data_15_to_30 <- explanatory_data %>% 
  mutate(price_twd_msq = predict(mdl_15_to_30, newdata = explanatory_data))

# Same again, with "30 to 45"
prediction_data_30_to_45 <- explanatory_data %>% 
  mutate(price_twd_msq = predict(mdl_30_to_45, newdata = explanatory_data))
```

## **Visualizing multiple models**

In the last two exercises, you ran models for each category of house ages separately, then calculated predictions for each model. Now it's time to visualize those predictions to see how they compare.

When you use `geom_smooth()` in a ggplot with an aesthetic that splits the dataset into groups and draws a line for each group (like the `color` aesthetic), you get multiple trend lines. This is the same as running a model on each group separately, so we get a chance to test our predictions against ggplot's.

`taiwan_real_estate`, `prediction_data_0_to_15`, `prediction_data_15_to_30`, and `prediction_data_30_to_45` are available; `ggplot2` is loaded.

**Instructions 1/2**

-   Using `taiwan_real_estate`, plot `price_twd_msq` versus `n_convenience` colored by `house_age_years`.

-   Add a point layer.

-   Add smooth trend lines for each color using the linear regression method and turning off the standard error ribbon.

```{r}
# Using taiwan_real_estate, plot price vs. no. of conv. stores colored by house age
ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) +
  # Make it a scatter plot
  geom_point() +
  # Add smooth linear regression trend lines, no ribbon
  geom_smooth(method = "lm", se = FALSE)
```

**2/2**:

-   Extend the plot by adding the prediction points from `prediction_data_0_to_15`. Color them red, with size 3 and shape 15.

-   Add prediction points from `prediction_data_15_to_30`, colored green, size 3, and shape 15.

-   Add prediction points from `prediction_data_30_to_45`, colored blue, size 3, and shape 15.

```{r}
# Extend the plot to include prediction points
ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # Add points using prediction_data_0_to_15, colored red, size 3, shape 15
  geom_point(data = prediction_data_0_to_15, color = "red", size = 3, shape = 15) +
  # Add points using prediction_data_15_to_30, colored green, size 3, shape 15
  geom_point(data = prediction_data_15_to_30, color = "green", size = 3, shape = 15) +
  # Add points using prediction_data_30_to_45, colored blue, size 3, shape 15
  geom_point(data = prediction_data_30_to_45, color = "blue", size = 3, shape = 15)
```

## **Assessing model performance**

To test which approach is best—the whole dataset model or the models for each house age category—you need to calculate some metrics. Here's, you'll compare the coefficient of determination and the residual standard error for each model.

Four models of price versus no. of convenience stores (`mdl_all_ages`, `mdl_0_to_15`, `mdl_15_to_30`, and `mdl_30_to_45`) are available; `dplyr` and `broom` are loaded.

**Instructions 1/2**

-   Get the coefficient of determination for `mdl_all_ages`, `mdl_0_to_15`, `mdl_15_to_30`, and `mdl_30_to_45`.

```{r}
mdl_all_ages <- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)

# Get the coeff. of determination for mdl_all_ages
mdl_all_ages %>%
    glance() %>%
    pull(r.squared)

# Get the coeff. of determination for mdl_0_to_15
mdl_0_to_15 %>%
    glance() %>%
    pull(r.squared)

# Get the coeff. of determination for mdl_15_to_30
mdl_15_to_30 %>%
    glance() %>% 
    pull(r.squared)

# Get the coeff. of determination for mdl_30_to_45
mdl_30_to_45 %>%
    glance() %>%
    pull(r.squared)
```

**2/2:** Get the residual standard error for `mdl_all_ages`, `mdl_0_to_15`, `mdl_15_to_30`, and `mdl_30_to_45`.

```{r}
# Get the RSE for mdl_all
mdl_all_ages %>%
    glance() %>%
    pull(sigma)

# Get the RSE for mdl_0_to_15
mdl_0_to_15 %>%
    glance() %>%
    pull(sigma)

# Get the RSE for mdl_15_to_30
mdl_15_to_30 %>%
    glance() %>%
    pull(sigma)

# Get the RSE for mdl_30_to_45
mdl_30_to_45 %>%
    glance() %>%
    pull(sigma)
```

## Video 2.2: One model with an interaction

Messing about with different models for different bits of your dataset is a pain. A better solution is to specify a single model that contains intercepts and slopes for each category. This is achieved through specifying interactions between explanatory variables.

### What is an interaction?

To understand the idea of interactions between explanatory variables, consider what we know about the fish dataset. Different fish species have different mass to length ratios. In statistical terms, we can say that the effect that length has on the expected mass of the fish varies between species. That means that length and species interact. More generally, if the effect of one explanatory variable on the expected response has different values dependent on the values of another explanatory variable, then those two explanatory variables interact.

> **In the fish dataset**
>
> The effect of length on the expected mass is different for different species.
>
> **More generally**
>
> The effect of one explanatory variable on the expected response changes depending on the value of another explanatory variable.

### Specifying interactions

You've seen how to include multiple explanatory variables in a formula using plus, for example, length plus species. To include an interaction between those variables, you simply swap the plus for a times. I've called this syntax implicit because you didn't write down what interactions are needed - R figures that out itself. Usually this concise syntax is best, but occasionally you may wish to explicitly document which interactions are included in the model. The explicit syntax is to add each explanatory variable separated by plus then add a third term with both explanatory variables separated by a colon. The result is exactly the same, so choosing a syntax depends on personal preference: do you like brevity or detail?

|  |  |
|----|----|
| **No Interactions** | **No Interactions** |
| `response ~ explntry1 + explntry2` | `mass_g ~ length_cm + species` |
| **With interactions (implicit)** | **With interactions (implicit)** |
| `response_var ~ explntry1 * explntry2` | `mass_g ~ length_cm * species` |
| **With interactions (explicit)** | **With interactions (explicit)** |
| `response ~ explntry1 + explntry2 + explntry1:explntry2` | `mass_g ~ length_cm + species + length_cm:species` |

### Running the model

Here's the formula in a model, and there are eight coefficients. As you saw in the models with a categorical explanatory variable, the coefficients are tricky to understand. The (Intercept) coefficient is the intercept for the first species, namely bream. The length coefficient is the slope for the bream. Then the (Intercept) coefficient plus the speciesPerch coefficient is the intercept for perch. And the length coefficient plus the length-speciesPerch coefficient is the slope for perch. It's a mess.

```{r}
lm(mass_g ~ length_cm * species, data = fish)
```

### Easier to understand coefficients

Ironically, to get easier to understand coefficients, we need to make the formula harder to read. This is the same model specified differently. On the right-hand side of the formula, you can see the categorical explanatory variable, species, then an interaction between the two explanatory variables, then zero to remove the global intercept. Now we get an intercept coefficient for each species shown on the top row, and a slope coefficient for each species shown on the bottom row.

```{r}
mdl_inter <- lm(mass_g ~ species + species:length_cm + 0, data = fish)
mdl_inter
```

### Familiar numbers

You've seen all these coefficient values before. If we examine the coefficients from the model on the bream data from the previous video, you can see that the intercept and slope are the same as in the model we just made. The same is true for the other three species. In fact, the model with the interaction is effectively the same as fitting separate models for each category, only you get the convenience of not having to manage four sets of code.

```{r}
coefficients(mdl_bream)
```

## **Specifying an interaction**

So far you used a single parallel slopes model, which gave an OK fit for the whole dataset, then three separate models for each house age category, which gave a better fit for each individual category, but was clunky because you had three separate models to work with and explain. Ideally, you'd have a single model that had all the predictive power of the individual models.

Defining this single model is achieved through adding interactions between explanatory variables. R's formula syntax is flexible, and gives you a couple of options, depending on whether you prefer concise code that is quick to type and to read, or explicit code that describes what you are doing in detail.

`taiwan_real_estate` is available.

**Instructions 1/2**

-   Fit a linear regression of `price_twd_msq` versus `n_convenience` and `house_age_years` and their interaction, using the "times" syntax to implicitly generate the interaction.

```{r}
# Model price vs both with an interaction using "times" syntax
lm(price_twd_msq ~ n_convenience * house_age_years, data = taiwan_real_estate)
```

**2/2:** fit a linear regression of `price_twd_msq` versus `n_convenience` and `house_age_years` and their interaction, using the "colon" syntax to explicitly generate the interaction.

```{r}
# Model price vs both with an interaction using "colon" syntax
lm(price_twd_msq ~ n_convenience + house_age_years + n_convenience:house_age_years, data = taiwan_real_estate)
```

## **Interactions with understandable coeffs**

The previous model with the interaction term returned coefficients that were a little tricky to interpret. In order clarify what the model is predicting, you can reformulate the model in a way that returns understandable coefficients. For further clarity, you can compare the results to the models on the separate house age categories (`mdl_0_to_15`, `mdl_15_to_30`, and `mdl_30_to_45`).

`taiwan_real_estate`, `mdl_0_to_15`, `mdl_15_to_30`, and `mdl_30_to_45` are available.

**Instructions 1/2**

-   Fit a linear regression of `price_twd_msq` versus `house_age_years` plus an interaction between `n_convenience` and `house_age_years`, and no global intercept, using the `taiwan_real_estate` dataset.

-   For comparison, get the coefficients for the three models for each category: `mdl_0_to_15`, `mdl_15_to_30`, and `mdl_30_to_45`.

```{r}
taiwan_0_to_15 <- taiwan_real_estate %>% 
  filter(house_age_years == "0 to 15")
taiwan_15_to_30 <- taiwan_real_estate %>% 
  filter(house_age_years == "15 to 30")
taiwan_30_to_45 <- taiwan_real_estate %>% 
  filter(house_age_years == "30 to 45")
```

```{r}
mdl_0_to_15 <- lm(formula = price_twd_msq ~ n_convenience, data = taiwan_0_to_15)
mdl_15_to_30 <- lm(formula = price_twd_msq ~ n_convenience, data = taiwan_15_to_30)
mdl_30_to_45 <- lm(formula = price_twd_msq ~ n_convenience, data = taiwan_30_to_45)
```

```{r}
# Model price vs. house age plus an interaction, no intercept
mdl_readable_inter <- lm(price_twd_msq ~ house_age_years + n_convenience:house_age_years + 0, data = taiwan_real_estate)

# See the result
coefficients(mdl_readable_inter)

# Get coefficients for mdl_0_to_15
coefficients(mdl_0_to_15)

# Get coefficients for mdl_15_to_30
coefficients(mdl_15_to_30)

# Get coefficients for mdl_30_to_45
coefficients(mdl_30_to_45)
```

## Video 2.3: Making predictions with interactions

Let's run through the prediction flow again, this time with the model containing an interaction. I'm hoping you get a sense of deja vu, because you've seen all this code before.

### The model with the interaction

Here's the model of mass versus length and species with an interaction. I've used the version of the formula that gives the clearest coefficients, but I could also have used the simpler length times species syntax; it doesn't affect the predictions.

```{r}
mdl_mass_vs_both_inter <- lm(mass_g ~ species + species:length_cm + 0, data = fish)
mdl_mass_vs_both_inter
```

### The prediction flow, again

Here's the code for the prediction flow. It's exactly the same as the code in the parallel slopes model. R will automatically take care of the interaction, so you don't need to change anything. I love it when things just work! The only thing to remember here is the use of `expand_grid` to get all the combinations of lengths and species.

```{r}
library(dplyr)
library(tidyr)
explanatory_data <- expand_grid(
  length_cm = seq(5, 60, 5),
  species = unique(fish$species)
)

prediction_data <- explanatory_data %>% 
  mutate(mass_g = predict(mdl_mass_vs_both_inter, explanatory_data))
```

### Visualizing the predictions

Here's the plot of predictions - both `ggplot`'s automatic prediction lines, and the points predicted from the model. The plot is identical to the one you saw in the first video of this chapter. This time however, the code is simpler because you don't have four separate models to worry about.

```{r}
ggplot(fish, aes(length_cm, mass_g, color = species)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = prediction_data, size = 3, shape = 15)
```

### Manually calculating the predictions

To see how the predictions work, let's manually calculate them. First we get the coefficients from the model using the coefficients function. Then we use square bracket indexing to extract the four intercepts and the four slopes.

```{r}
coeffs <- coefficients(mdl_mass_vs_both_inter)

intercept_bream <- coeffs[1]
intercept_perch <- coeffs[2]
intercept_pike <- coeffs[3]
intercept_roach <- coeffs[4]
slope_bream <- coeffs[5]
slope_perch <- coeffs[6]
slope_pike <- coeffs[7]
slope_roach <- coeffs[8]

```

```{r}
explanatory_data %>%  
  mutate(    
    mass_g = case_when(      
      species == "Bream" ~ intercept_bream + slope_bream * length_cm,      
      species == "Perch" ~ intercept_perch + slope_perch * length_cm,      
      species == "Pike" ~ intercept_pike + slope_pike * length_cm,      
      species == "Roach" ~ intercept_roach + slope_roach * length_cm
      )
    )

```

## **Predicting with interactions**

As with every other regression model you've created, the fun part is making predictions. Fortunately, the code flow for this case is the same as the one without interactions—R can handle calculating the interactions without any extra prompting from you. The only thing you need to remember is the trick for getting combinations of explanatory variables.

`mdl_price_vs_both_inter` is available; `dplyr` and `ggplot2` are loaded.

**Instructions 1/3**

Make a grid of explanatory data, formed from combinations of the following variables.

-   `n_convenience` should take the numbers zero to ten.

-   `house_age_years` should take the unique values of the `house_age_years` column of `taiwan_real_estate`.

```{r}
mdl_price_vs_both_inter <- lm(formula = price_twd_msq ~ house_age_years + n_convenience:house_age_years + 0, data = taiwan_real_estate)
```

```{r}
# Make a grid of explanatory data
explanatory_data <- expand_grid(
  # Set n_convenience to zero to ten
  n_convenience = 0:10,
  # Set house_age_years to the unique values of that variable
  house_age_years = unique(taiwan_real_estate$house_age_years)
)

# See the result
explanatory_data
```

**2/3**

-   Add a column to the `explanatory_data`, assigning to `prediction_data`.

-   The column should be named after the response variable, and contain predictions made using `mdl_price_vs_both_inter` and `explanatory_data`.

```{r}
# From previous step
explanatory_data <- expand_grid(
  n_convenience = 0:10,
  house_age_years = unique(taiwan_real_estate$house_age_years)
)

# Add predictions to the data frame
prediction_data <- explanatory_data %>%
  mutate(price_twd_msq = predict(mdl_price_vs_both_inter, explanatory_data))

# See the result
prediction_data
```

**3/3**

-   Using `taiwan_real_estate`, plot `price_twd_msq` versus `n_convenience`, colored by `house_age_years`.

-   Add a point layer.

-   Add smooth trend lines using linear regression, no standard error ribbon.

-   Add another point layer using `prediction_data`, with `size` `5` and `shape` `15`.

```{r}
# Using taiwan_real_estate, plot price vs. no. of convenience stores, colored by house age
ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) +
  # Make it a scatter plot
  geom_point() +
  # Add linear regression trend lines, no ribbon
  geom_smooth(method = "lm", se = FALSE) +
  # Add points from prediction_data, size 5, shape 15
  geom_point(data = prediction_data, size = 5, shape = 15)
```

## **Manually calculating predictions with interactions**

In order to understand how `predict()` works, it's time to calculate the predictions manually again. For this model, there are three separate lines to calculate for, and in each one, the prediction is an intercept plus a slope times the numeric explanatory value. The tricky part is getting the right intercept and the right slope for each case.

`mdl_price_vs_both_inter` and `explanatory_data` are available; `dplyr` and `tidyr` are available.

**Instructions 1/2**

-   Get the coefficients from `mdl_price_vs_both_inter`, assigning to `coeffs`.

-   Get the three intercept coefficients from `coeffs`, assigning to `intercept_0_15`, `intercept_15_30`, and `intercept_30_45`.

-   Get the three slope coefficients from `coeffs`, assigning to `slope_0_15`, `slope_15_30`, and `slope_30_45`.

```{r}
mdl_price_vs_both_inter <- lm(formula = price_twd_msq ~ house_age_years + n_convenience:house_age_years + 0, data = taiwan_real_estate)

# Get the coefficients from mdl_price_vs_both_inter
coeffs <- coefficients(mdl_price_vs_both_inter)

# Get the intercept for 0 to 15 year age group
intercept_0_15 <- coeffs[1]

# Get the intercept for 15 to 30 year age group
intercept_15_30 <- coeffs[2]

# Get the intercept for 30 to 45 year age group
intercept_30_45 <- coeffs[3]

# Get the slope for 0 to 15 year age group
slope_0_15 <- coeffs[4]

# Get the slope for 15 to 30 year age group
slope_15_30 <- coeffs[5]

# Get the slope for 30 to 45 year age group
slope_30_45 <- coeffs[6]
```

**2/2** Add a `price_twd_msq` column to `explanatory_data` containing the predictions.

-   In the case when `house_age_years` is `"0 to 15"`, choose the appropriate intercept plus the appropriate slope times the number of nearby convenience stores.

-   Do likewise for the cases where the house age is `"15 to 30"` and `"30 to 45"`.

```{r}
# From previous step
coeffs <- coefficients(mdl_price_vs_both_inter)
intercept_0_15 <- coeffs[1]
intercept_15_30 <- coeffs[2]
intercept_30_45 <- coeffs[3]
slope_0_15 <- coeffs[4]
slope_15_30 <- coeffs[5]
slope_30_45 <- coeffs[6]

prediction_data <- explanatory_data %>% 
  mutate(
    # Consider the 3 cases to choose the price
    price_twd_msq = case_when(
      house_age_years == "0 to 15" ~ intercept_0_15 + slope_0_15 * n_convenience,
      house_age_years == "15 to 30" ~ intercept_15_30 + slope_15_30 * n_convenience,
      house_age_years == "30 to 45" ~ intercept_30_45 + slope_30_45 * n_convenience
    )
  )

# See the result
prediction_data
```

## Video 2.4: Simpson's Paradox

This chapter looked at the difference between models of a whole dataset and individual models for each category. For some datasets, this can lead to a nonintuitive result known as Simpson's Paradox.

### A most ingenious paradox!

Simpson's Paradox occurs when the trend of a model on the whole dataset is very different from the trends shown by models on subsets of the dataset.

*trend = slope coefficient*

### Synthetic Simpson data

Here's a synthetic - that is, made up - dataset designed to demonstrate the paradox. Each row has an x and y coordinate, and the dataset is split into five groups, labeled A to E.

```{r}
head(simpsons_paradox)
```

### Linear regressions

Fitting a linear regression of y versus x to the whole dataset shows a positive slope of one point seven five. However, fitting a model that includes the group and an interaction shows something completely different. The bottom row of coefficients contains the slope for each group. Every group has a negative slope, apparently contradicting the fact that the whole dataset has a positive slope. Let's visualize the dataset to try and reconcile these opposite coefficients.

**Whole dataset**

```{r}
mdl_whole <- lm(
  y ~ x,
  data = simpsons_paradox
)
coefficients(mdl_whole)
```

**By group**

```{r}
mdl_by_group <- lm(
  y ~ group + group:x + 0,
  data = simpsons_paradox
)
coefficients(mdl_by_group)
```

### Plotting the whole dataset

This is the now-standard scatter plot with a linear regression trend line. As x increases, so does y, resulting in a positive slope over the whole dataset.

```{r}
ggplot(simpsons_paradox, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

### Plotting by group

Amending the plot to color the lines by group shows that within each group, y increases as x increases.

```{r}
ggplot(simpsons_paradox, aes(x, y, color = group)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

### Reconciling the difference

-   **Good advice**: If possible, try to plot the dataset.

-   **Common advice**: You can't choose the best model in general–it depends on the dataset and the question you are trying to answer.

-   **More good advice**: Articulate a question before you start modeling.

### Test score example

Thinking up examples where the grouped model is best is fairly easy. Here's the same synthetic dataset as before, with different axis labels. If x is the number of hours spent playing video games each month, and y is the score on a test, modeling the whole dataset suggests that playing more video games is related to a higher test score. If we reveal that each group represents the age of the child taking the test, it changes the interpretation. Now older children score more highly in the test, and playing lots of video games is related to a lower score.

### Infectious disease example

Coming up with examples where the model of the whole dataset is more useful than the model split by group is much harder. So much so that I had to ask the internet for suggestions. One proposed example was that for an infectious disease, the infection rate tends to be higher when the population density is higher. In this plot, each point represents a neighborhood in a city. Splitting by city reveals that the highest density areas of each city have lower infection rates. However, this may be due to other things that you haven't included in the model, like the wealth and demographics of the residents. That's an interesting insight, but "increasing population density is related to increased infection rate" is arguably more important.

### Reconciling the difference, again

Unfortunately, resolving these model disagreements is messy. Often, the models including the groups will contain insight that you'd miss otherwise. The disagreements between the models may reveal that you need even more explanatory variables to understand why they are different. Finally, I'm going to repeat the correct but annoying advice: to choose the best model you need contextual information about what your dataset means and what question you are trying to answer.

### Simpson's paradox in real datasets

Such a clear case of Simpson's paradox is very rare. Subtle differences between models are more common. A slope may go to zero instead of changing its direction. You may only see the effect in some groups, but not all of them.

## **Modeling eBay auctions**

Sometimes modeling a whole dataset suggests trends that disagree with models on separate parts of that dataset. This is known as Simpson's paradox. In the most extreme case, you may see a positive slope on the whole dataset, and negative slopes on every subset of that dataset (or the other way around).

Over the next few exercises, you'll look at [**eBay auctions**](http://www.modelingonlineauctions.com/datasets) of Palm Pilot M515 PDA models.

| variable       | meaning                        |
|:---------------|:-------------------------------|
| `price`        | Final sale price, USD          |
| `openbid`      | The opening bid, USD           |
| `auction_type` | How long did the auction last? |

`auctions` is available; `dplyr` and `ggplot2` are loaded.

**Instructions 1/2**

-   Look at the structure of the `auctions` dataset and familiarize yourself with its columns.

-   Fit a linear regression model of `price` versus `openbid`, using the `auctions` dataset. *Look at the coefficients.*

```{r}
# Take a glimpse at the dataset
glimpse(auctions)

# Model price vs. opening bid using auctions
mdl_price_vs_openbid <- lm(price ~ openbid, data = auctions)

# See the result
mdl_price_vs_openbid
```

**2/2**: Using auctions, plot `price` versus `openbid` as a scatter plot with linear regression trend lines (no ribbon). *Look at the trend line.*

```{r}
# Using auctions, plot price vs. opening bid as a scatter plot with linear regression trend lines
ggplot(auctions, aes(openbid, price)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
```

## **Modeling each auction type**

You just saw that the opening bid price appeared not to affect the final sale price of Palm Pilots in the eBay auctions. Now let's look at what happens when you model the three auction types (3 day, 5 day, and 7 day) separately.

`auctions` is available; `dplyr` and `ggplot2` are loaded.

**Instructions 1/3**

-   Fit a linear regression model of `price` versus `openbid` and `auction_type`, with an interaction, using the `auctions` dataset. *Look at the coefficients.*

```{r}
# Fit linear regression of price vs. opening bid and auction type, with an interaction.
mdl_price_vs_both <- lm(price ~ openbid * auction_type, data = auctions)

# See the result
mdl_price_vs_both
```

**2/3**

-   Using `auctions`, plot `price` versus `openbid`, colored by `auction_type`, as a scatter plot with linear regression trend lines (no ribbon). *Look at the trend lines.*

```{r}
# Using auctions, plot price vs. opening bid colored by auction type as a scatter plot with linear regr'n trend lines
ggplot(auctions, aes(openbid, price, color = auction_type)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
```
